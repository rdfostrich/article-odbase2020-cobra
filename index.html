<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title property="foaf:name schema:name">Optimizing Storage of RDF Archives using Forward and Reverse Deltas</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  <meta name="citation_title" content="Optimizing Storage of RDF Archives using Forward and Reverse Deltas">
  <meta name="citation_author" content="Ruben Taelman" />
  <meta name="citation_author" content="Thibault Mahieu" />
  <meta name="citation_author" content="Ruben Verborgh" />
  
  <meta name="citation_publication_date" content="2020/07/07" />
</head>

<body prefix="dctypes: http://purl.org/dc/dcmitype/ pimspace: http://www.w3.org/ns/pim/space# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# lsc: http://linkedscience.org/lsc/ns#" typeof="schema:CreativeWork sioc:Post prov:Entity lsc:Research">
  <header>
  <h1 id="optimizing-storage-of-rdf-archives-using-forward-and-reverse-deltas">Optimizing Storage of RDF Archives using Forward and Reverse Deltas</h1>

  <ul id="authors">
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="http://www.rubensworks.net/" typeof="foaf:Person schema:Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="#" typeof="foaf:Person schema:Person" resource="#">Thibault Mahieu</a></li>
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://ruben.verborgh.org/" typeof="foaf:Person schema:Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
  </ul>

  <ul id="affiliations">
    <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
  </ul>

  <section class="context">
    <h2 id="in-reply-to">In reply to</h2>
    <ul>
      <li><a href="https://linkedresearch.org/calls" rel="as:inReplyTo">Call for Linked Research</a></li>
      <li><a href="http://www.otmconferences.org/index.php/conferences/odbase-2020" rel="as:inReplyTo">ODBASE 2020 call for papers</a></li>
    </ul>
  </section>

  <section id="abstract" inlist="" rel="schema:hasPart" resource="#abstract">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Abstract</h2>
      <!-- Context      -->
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
<!-- Need         -->
Vestibulum finibus dignissim augue, id pellentesque est facilisis non.
<!-- Task         -->
Donec fringilla dolor non neque iaculis blandit.
<!-- Object       -->
Praesent aliquet eleifend iaculis.
<!-- Findings     -->
Quisque pellentesque at odio ac bibendum.
<!-- Conclusion   -->
Pellentesque imperdiet felis urna, quis facilisis lacus gravida non.
<!-- Perspectives -->
Donec quis lectus eget sem tempor tristique pellentesque in dolor.</p>

    </div>
</section>

</header>

<main>
  <!-- Add sections by specifying their file name, excluding the '.md' suffix. -->
  <section id="introduction" inlist="" rel="schema:hasPart" resource="#introduction">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Introduction</h2>

      <ul class="todo">
        <li>Motivate RDF archiving</li>
        <li>Briefly explain basics around ostrich</li>
        <li>Point to problems of ostrich with long delta chains</li>
        <li>Say that we will attempt to solve this using forward and reverse delta chains</li>
      </ul>

    </div>
</section>

  <section id="related-work" inlist="" rel="schema:hasPart" resource="#related-work">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Related Work</h2>

      <p>In this section, we discuss the fundamentals on RDF archiving,
which RDF archiving solutions already exist,
and benchmarks for RDF archiving.
Finally, we discuss OSTRICH in more detail,
since we build upon this approach in this work.</p>

      <h3 id="rdf-archiving">RDF Archiving</h3>

      <p>Various techniques have been introduced to store <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">RDF datasets</a> <span class="references">[<a href="#ref-1">1</a>, <a href="#ref-2">2</a>]</span>.
These techniques make use of various indexing and compression techniques
to optimize query execution and storage size.
Since RDF datasets typically change over time <span class="references">[<a href="#ref-3">3</a>]</span>,
there is a need to <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">maintain the history of these datasets</a> <span class="references">[<a href="#ref-4">4</a>]</span>,
which gave rise to the research domain of <em>RDF archiving</em>.</p>

      <p>An <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><em>RDF archive</em></a> <span class="references">[<a href="#ref-5">5</a>]</span> has been defined as follows:
<em>An RDF archive graph A is a set of version-annotated triples.</em>
Where a <em>version-annotated triple</em> <em>(s, p, o):[i]</em> is defined as <em>an RDF triple (s, p, o) with a label i ∈ N representing the version in which this triple holds.</em>
The set of all <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF triples</a> <span class="references">[<a href="#ref-6">6</a>]</span> is defined as <em>(U ∪ B) × U × (U ∪ B ∪ L)</em>,
where <em>U</em>, <em>B</em>, and <em>L</em>, respectively represent the disjoint, infinite sets of URIs, blank nodes, and literals.
Furthermore,
<em>an RDF version of an RDF archive A at snapshot i is the RDF graph A(i) = {(s, p, o)|(s, p, o):[i] ∈ A}.</em>
For the remainder of this article, we use the notation <em>V<sub>i</sub></em> to refer to the RDF version <em>A(i)</em>.</p>

      <p>RDF archives allow multiple versions to exist in parallel,
which leads to a range of new querying possibilities.
Instead of only querying within the latest version of a dataset,
also previous versions can be queried,
or even differences between different versions.
To cover this new range of querying possibilities,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">five foundational query types were introduced</a> <span class="references">[<a href="#ref-5">5</a>]</span>,
which are referred to as <em>query atoms</em>.
These query atoms make use of concepts from the
the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">RDF data model</a> <span class="references">[<a href="#ref-6">6</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/">SPARQL query language</a> <span class="references">[<a href="#ref-7">7</a>]</span>.
In these models, a <em>triple pattern</em> is defined as <em>(U ∪ V) × (U ∪ V) × (U ∪ L ∪ V)</em>, with <em>V</em> being the infinite set of variables.
A set of triple patterns is called a <em>Basic Graph Pattern</em>, which forms the basis of a SPARQL query.
The evaluation of a SPARQL query <em>Q</em> on an RDF graph <em>G</em> containing RDF triples,
produces a bag of solution mappings <em>[[Q]]<sub>G</sub></em>.
The five query atoms are defined as follows:</p>

      <ol>
        <li><strong>Version materialization (VM)</strong> retrieves data using a query <em>Q</em> targeted at a single version <em>V<sub>i</sub></em>.
Formally: <em>VM(Q, V<sub>i</sub>) = [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were present in the library yesterday?</em></li>
        <li><strong>Delta materialization (DM)</strong> retrieves query <em>Q</em>’s result change sets between two versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em>DM(Q, V<sub>i</sub>, V<sub>j</sub>)=(Ω<sup>+</sup>, Ω<sup>−</sup>). With Ω<sup>+</sup> = [[Q]]<sub>V<sub>i</sub></sub> \ [[Q]]<sub>V<sub>j</sub></sub> and Ω<sup>−</sup> = [[Q]]<sub>V<sub>j</sub></sub> \ [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were returned or taken from the library between yesterday and now?</em></li>
        <li><strong>Version query (VQ)</strong> annotates query <em>Q</em>’s results with the versions (of RDF archive A) in which they are valid.
Formally: <em>VQ(Q, A) = {(Ω, W) | W = {A(i) | Ω=[[Q]]<sub>A(i)</sub>, i ∈ N} ∧ Ω ≠ ∅}</em>.
Example: <em>At what times was book X present in the library?</em></li>
        <li><strong>Cross-version join (CV)</strong> joins the results of two queries (<em>Q1</em> and <em>Q2</em>) between versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em>VM(Q1, V<sub>i</sub>) ⨝ VM(Q2, V<sub>j</sub>)</em>.
Example: <em>What books were present in the library yesterday and today?</em></li>
        <li><strong>Change materialization (CM)</strong> returns a list of versions in which a given query <em>Q</em> produces
consecutively different results.
Formally: <em>{(i, j) | i,j ∈ ℕ, i &lt; j, DM(Q, A(i), A(j)) = (Ω<sup>+</sup>, Ω<sup>−</sup>), Ω<sup>+</sup> ∪ Ω<sup>−</sup> ≠ ∅, ∄ k ∈ ℕ : i &lt; k &lt; j}</em>.
Example: <em>At what times was book X returned or taken from the library?</em></li>
      </ol>

      <p>In the remainder of this article, we focus on version materialization (VM), delta materialization (DM), and version (VQ) queries,
as CV and CM queries can be expressed in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">terms of the other ones</a> <span class="references">[<a href="#ref-8">8</a>]</span>.</p>

      <h3 id="rdf-archiving-solutions">RDF Archiving Solutions</h3>

      <p>In the recent years, several techniques and solutions have been proposed to allow storing and querying RDF archives.
RDF archiving systems are typically categorized into <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">three non-orthogonal storage strategies</a> <span class="references">[<a href="#ref-4">4</a>]</span>:</p>

      <ul>
        <li>The <strong>Independent Copies (IC)</strong> approach creates separate instantiations of datasets for
each change or set of changes.</li>
        <li>The <strong>Change-Based (CB)</strong> approach instead only stores change sets between versions.</li>
        <li>The <strong>Timestamp-Based (TB)</strong> approach stores the temporal validity of facts.</li>
      </ul>

      <p>There exists a correspondence between these query atoms
and the independent copies (IC), change-based (CB), and timestamp-based (TB) storage strategies.
Namely, IC typically leads to efficient VM queries,
CB is better for DM queries,
and TB is best for VQ queries.
No single strategy leads to good performance of all query atoms.</p>

      <p><a href="#rdf-archive-systems">Table 1</a> shows an overview of the primary RDF archiving systems,
and which storage strategy they follow.</p>

      <figure id="rdf-archive-systems" class="table">

        <table>
          <thead>
            <tr>
              <th>Name</th>
              <th>IC</th>
              <th>CB</th>
              <th>TB</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>SemVersion <span class="references">[<a href="#ref-9">9</a>]</span></td>
              <td>✓</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>Cassidy et. al. <span class="references">[<a href="#ref-10">10</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>R&amp;WBase <span class="references">[<a href="#ref-11">11</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>R43ples <span class="references">[<a href="#ref-12">12</a>]</span></td>
              <td> </td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td>Hauptman et. al. <span class="references">[<a href="#ref-13">13</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>X-RDF-3X <span class="references">[<a href="#ref-14">14</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf">RDF-TX</a> <span class="references">[<a href="#ref-15">15</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>v-RDFCSA <span class="references">[<a href="#ref-16">16</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td>Dydra <span class="references">[<a href="#ref-17">17</a>]</span></td>
              <td> </td>
              <td> </td>
              <td>✓</td>
            </tr>
            <tr>
              <td><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2018.08.002"><a href="https://arxiv.org/pdf/1805.03721">Quit Store</a></span> <span class="references">[<a href="#ref-18">18</a>]</span></td>
              <td>✓</td>
              <td> </td>
              <td> </td>
            </tr>
            <tr>
              <td>TailR <span class="references">[<a href="#ref-19">19</a>]</span></td>
              <td>✓</td>
              <td>✓</td>
              <td> </td>
            </tr>
            <tr>
              <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">OSTRICH</a> <span class="references">[<a href="#ref-20">20</a>]</span></td>
              <td>✓</td>
              <td>✓</td>
              <td>✓</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 1:</span> Overview of RDF archiving solutions with their corresponding storage strategy:
Individual copies (IC), Change-based (CB), or Timestamp-based (TB).</p>
        </figcaption>
      </figure>

      <h4 id="independent-copies-approaches">Independent copies approaches</h4>
      <p>SemVersion <span class="references">[<a href="#ref-9">9</a>]</span> tracks different versions of RDF graphs,
using Concurrent Versions System (CVS) concepts to maintain different versions of ontologies,
such as diff, branching and merging.
<span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2018.08.002"><a href="https://arxiv.org/pdf/1805.03721">Quit Store</a></span> <span class="references">[<a href="#ref-18">18</a>]</span> is a system that is built on top of Git,
which allows these same features by considering each version to be a commit.</p>

      <h4 id="change-based-approaches">Change-based approaches</h4>
      <p>Cassidy et. al. <span class="references">[<a href="#ref-10">10</a>]</span> propose a system to store changes to graphs as a series of patches, which makes it a CB approach.
They describe operations on versioned graphs such as reverse, revert and merge.
A preliminary evaluation shows that their implementation is significantly slower
than a native RDF store.
Im et. al. <span class="references">[<a href="#ref-21">21</a>]</span> propose a CB patching system based on a relational database.
In their approach, they use a storage scheme called <em>aggregated deltas</em>
which associates the latest version with each of the previous ones.
While aggregated deltas result in fast delta queries, they introduce a higher storage overhead.
R&amp;WBase <span class="references">[<a href="#ref-11">11</a>]</span> is a CB versioning system that uses the graph component of quad-stores to build a versioning layer.
It supports tagging, branching and merging.
R43ples <span class="references">[<a href="#ref-12">12</a>]</span> follows a similar approach to R&amp;WBase,
but they additionally introduce new SPARQL keywords, such as REVISION, BRANCH and TAG.</p>

      <h4 id="timestamp-based-approaches">Timestamp-based approaches</h4>
      <p>Hauptman et. al. <span class="references">[<a href="#ref-13">13</a>]</span> store each triple in a different named graph as a TB storage approach.
The identifying graph of each triple is used in a commit graph for SPARQL query evaluation at a certain version.
X-RDF-3X <span class="references">[<a href="#ref-14">14</a>]</span> is a versioning extension of RDF-3X <span class="references">[<a href="#ref-2">2</a>]</span>,
where each triple is annotated with a creation and deletion timestamp.
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf">RDF-TX</a> <span class="references">[<a href="#ref-15">15</a>]</span> is an in-memory query engine that supports a temporal SPARQL querying extension,
which makes use of a compressed multi-version B+Trees that outperforms similar systems such as X-RDF-3X in terms of querying efficiency,
while having similar storage requirements.
v-RDFCSA <span class="references">[<a href="#ref-16">16</a>]</span> is a self-indexing RDF archive mechanism,
that enables versioning queries on top of compressed RDF archives as a TB approach.
Dydra <span class="references">[<a href="#ref-17">17</a>]</span> is an RDF graph storage platform with dataset versioning support.
They introduce the REVISION keyword, which is similar to the GRAPH SPARQL keyword for referring to different dataset versions.</p>

      <h4 id="hybrid-approaches">Hybrid approaches</h4>
      <p>TailR <span class="references">[<a href="#ref-19">19</a>]</span> is an HTTP archive for Linked Data pages for retrieving prior versions of certain HTTP resources.
It is a hybrid CB/IC approach as it starts by storing a dataset snapshot,
after which only deltas are stored for each consecutive version, as shown in <a href="#regular-delta-chain"></a>.
When the chain becomes too long, or other conditions are fulfilled,
a new snapshot is created for the next version to avoid long version reconstruction times.
<a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">OSTRICH</a> <span class="references">[<a href="#ref-20">20</a>]</span> is a hybrid IC/CB/TB approach that exploits the advantages of each strategy
to provide a trade-off between storage requirements and querying efficiency.
Experiments show that OSTRICH achieves good querying performance for all query atoms,
but suffers from scalability issues in terms of ingestion time for many versions.
As such, we build upon OSTRICH in this work, and attempt to solve this problem.</p>

      <h3 id="rdf-archiving-benchmarks">RDF Archiving Benchmarks</h3>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">BEAR</a> <span class="references">[<a href="#ref-5">5</a>]</span> is a benchmark for RDF archive systems.
that is based on three real-world datasets from different domains:</p>

      <dl>
        <dt>BEAR-A</dt>
        <dd>58 weekly snapshots from the Dynamic Linked Data Observatory <span class="references">[<a href="#ref-3">3</a>]</span>.</dd>
        <dt>BEAR-B</dt>
        <dd>The 100 most volatile resources from DBpedia Live <span class="references">[<a href="#ref-22">22</a>]</span> over the course of three months
as three different granularities: instant, hour and day.</dd>
        <dt>BEAR-C</dt>
        <dd>Dataset descriptions from the Open Data Portal Watch <span class="references">[<a href="#ref-23">23</a>]</span> project over the course of 32 weeks.</dd>
      </dl>

      <p>The 58 versions of BEAR-A contain between 30M and 66M triples per version, with an average change ratio of 31%.
BEAR-A provides triple pattern queries for three different query atoms for both result sets with a low and a high cardinality.
BEAR-B provides a small collection of triple pattern queries corresponding to the real-world usage of DBpedia.
Finally, BEAR-C provides 10 complex SPARQL queries that were created with the help of Open Data experts.</p>

      <p>BEAR provides baseline RDF archive implementations based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-1">1</a>]</span> and Jena <span class="references">[<a href="#ref-24">24</a>]</span>
for the IC, CB, and TB approaches, but also hybrid IC/CB and TB/CB approaches.
The hybrid approaches are based on snapshots followed by delta chains, as implemented by TailR <span class="references">[<a href="#ref-19">19</a>]</span>.</p>

      <p>Due to BEAR covering all query atoms we work with,
and it providing baseline implementations for the different storage strategies,
we make use of BEAR for our experiments.</p>

      <h3 id="ostrich">OSTRICH</h3>

      <p>As mentioned before, <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">OSTRICH</a> <span class="references">[<a href="#ref-20">20</a>]</span> make us of a hybrid IC/CB/TB storage approach
with the goal of providing a trade-off between storage size and querying efficiency.
The main motivation for OSTRICH is to serve as a back-end of a <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf">low-cost Web APIs for exposing RDF archives</a> <span class="references">[<a href="#ref-25">25</a>]</span>,
where query execution must be sufficiently fast,
without requiring too much storage.</p>

      <p>Concretely, OSTRICH always starts by storing the initial version as a fully materialized version, following the IC strategy.
This initial version is stored using <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-1">1</a>]</span>, which enables high compression and efficient querying.
Based on this initial version, all following versions are stored as deltas, following the CB strategy.
To solve the problem of increasing query execution times for increasing numbers of versions,
OSTRICH makes use of the aggregated deltas <span class="references">[<a href="#ref-21">21</a>]</span> approach,
by making each delta relative to the initial snapshot instead of the previous version.
Due to the storage redundancies that are introduced because of these aggregated deltas,
OSTRICH uses a B+tree-based approach to store all aggregated deltas in a single store.
This single store annotates each added and deleted triple with the delta version in which it exists,
thereby following the timestamp-based strategy.
To further reduce storage requirements and query execution times,
all triple components inside this store are dictionary-encoded, similar to the approach followed by HDT.</p>

      <p>On top of this storage approach, OSTRICH introduces algorithms for VM, DM and VQ triple pattern queries.
Only triple pattern queries are supported instead of full SPARQL queries,
since triple pattern queries are the foundational building blocks for more expressive SPARQL queries.
These query algorithms produce streaming results, where the streams can start from an arbitrary offset,
which is valuable for SPARQL query features such as <code>OFFSET</code> and <code>LIMIT</code>.
Additionally, OSTRICH provides algorithms for cardinality estimation for these queries,
which are valuable for query planning within query engines.
OSTRICH has been implemented in <a href="https://github.com/rdfostrich/ostrich">C/C++</a>,
with bindings existing for <a href="https://github.com/rdfostrich/ostrich-node">Node.JS (JavaScript)</a>.
The triple pattern index provided by OSTRICH has been demonstrated
to be usable within a full SPARQL query engine such as <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-mocha-2018/">Comunica</a> <span class="references">[<a href="#ref-26">26</a>, <a href="#ref-27">27</a>]</span>.</p>

      <p>Experimental results on OSTRICH with the BEAR benchmark show that this hybrid strategy
is more beneficial than having just a single storage strategy,
as it allows efficient execution of all query atoms.
The main downside of this approach is that it leads to scalability issues in terms of ingestion time for many versions.
Concretely, the BEAR-B-hourly dataset—which contains 1299 versions—
starts showing high ingestion times starting around version 1100.
The reason for this is that the aggregated deltas start becoming too large.
As such, we build upon OSTRICH in this work, and attempt to solve this problem by modifying the delta chain structure.</p>

    </div>
</section>

  <section id="related-work" inlist="" rel="schema:hasPart" resource="#related-work">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Problem Statement</h2>

      <p>As mentioned in <a href="#introduction">Section 1</a>, the hybrid storage solution provided by OSTRICH leads to long delta chains,
which can become problematic for RDF archives with many versions in terms of ingestion time and storage size.
Our goal in this work is to investigate if these issues can be alleviated by modifying the delta chain structure.</p>

      <p>We formulate our research question as follows:
<q id="research-question">How can we improve the storage of RDF archives under the hybrid storage strategy by modification of the delta chain structure?</q></p>

      <p>Concretely, we start from the storage approach from OSTRICH, and we apply two modifications:</p>

      <ol>
        <li>Creation of new snapshots, which leads to multiple delta chains.</li>
        <li>Introduction of bidirectional delta chains using reverse deltas.</li>
      </ol>

      <p>Since both of these modifications will reduce the length of delta chains,
we expect that this will reduce ingestion time, overall storage size, and average query execution time for all query atoms.
Based on this, we define the following hypotheses:</p>

      <ol>
        <li id="hypothesis-qualitative-storage">Storage size will be significantly lower for a bidirectional delta chain compared to a unidirectional delta chain.</li>
        <li id="hypothesis-qualitative-ingestion">In-order ingestion time will be lower for a unidirectional delta chain compared to a bidirectional delta chain.</li>
        <li id="hypothesis-qualitative-querying-vm">The mean VM query execution will be lower for a unidirectional delta chain compared to a bidirectional delta chain.</li>
        <li id="hypothesis-qualitative-querying-dm">The mean DM query execution will be lower for a unidirectional delta chain compared to a bidirectional delta chain.</li>
        <li id="hypothesis-qualitative-querying-vq">The mean VQ query execution will be lower for a unidirectional delta chain compared to a bidirectional delta chain.</li>
      </ol>

    </div>
</section>

  <section id="solution" inlist="" rel="schema:hasPart" resource="#solution">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Bidirectional Delta Chain</h2>

      <p>In this section, we explain our bidirectional delta chain approach.
We start by explaining the general idea behind a bidirectional delta chain.
After that, we explain its implication on storage.
Finally, we discuss querying algorithms for the foundational query atoms based on this storage approach.</p>

      <h3 id="solution-approaches">Delta Chain Approaches</h3>

      <p>In the scope of this work, we distinguish between six different delta chain approaches,
as can be seen in <a href="#delta-chain-approaches">Table 2</a>.
We decompose these approaches into 2 axes: directionality and aggregation.</p>

      <p>In terms of directionality, we distinguish three forms.
The simplest form is the <em>forward unidirectional</em> delta chain,
where the snapshot comes first, and is followed by deltas that are relative to the previous delta.
The <em>reverse unidirectional</em> delta chain is a variant of this where everything is reversed.
Concretely, the snapshot comes last, and is preceded by deltas, where each delta is relative to the next delta.
These forward and reverse unidirectional approaches can be combined with each other to form a <em>bidirectional delta chain</em>,
where a first set of deltas exist before the snapshot,
and a second set of deltas exists after the snapshot.</p>

      <p>In terms of aggregation, we consider the default non-aggregated form and the aggregated form.
In the non-aggregated form, each delta is relative to the delta immediately before or after it.
In the aggregated form <span class="references">[<a href="#ref-21">21</a>]</span>, each delta is relative to the snapshot before or after it,
where other deltas may occur inbetween.
This aggregated delta approach leads to lower version materialization times,
since each delta can be directly applied to a snapshot,
as opposed to non-aggregated deltas where multiple deltas need to be combined before a version can be materialized.
As such, the version materialization time for aggregated deltas is <code>O(1)</code> with respect to the number of versions,
while it is <code>O(n)</code> for non-aggregated deltas with respect to the number of versions.
This shows how aggregated deltas lead to better query execution times.
The major downside of aggregated deltas is however that storage size increases due to the redundancies between the different deltas.
The longer the delta chain, the larger these redundancies become.</p>

      <p>In the context of this work,
OSTRICH is an example that follows the unidirectional forward aggregated delta chain approach,
while RCS <span class="references">[<a href="#ref-28">28</a>]</span> (non-RDF-based) follows the unidirectional reverse non-aggregated delta chain approach.
In this work, we will make use of the bidirectional aggregated delta chain approach,
which we will explain in the next section.</p>

      <figure id="delta-chain-approaches" class="table">

        <table class="delta-approaches">
          <thead>
            <tr>
              <th> </th>
              <th><strong>Non-aggregated</strong></th>
              <th><strong>Aggregated</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Forward Unidirectional</strong></td>
              <td><img src="img/delta-chain-uni.svg" alt="Unidirectional delta chain" class="delta-approach" /></td>
              <td><img src="img/delta-chain-uni-agg.svg" alt="Unidirectional aggregated delta chain" class="delta-approach" /></td>
            </tr>
            <tr>
              <td><strong>Reverse Unidirectional</strong></td>
              <td><img src="img/delta-chain-uni-rev.svg" alt="Unidirectional reverse delta chain" class="delta-approach" /></td>
              <td><img src="img/delta-chain-uni-agg-rev.svg" alt="Unidirectional aggregated reverse delta chain" class="delta-approach" /></td>
            </tr>
            <tr>
              <td><strong>Bidirectional</strong></td>
              <td><img src="img/delta-chain-bi.svg" alt="Bidirectional delta chain" class="delta-approach" /></td>
              <td><img src="img/delta-chain-bi-agg.svg" alt="Bidirectional aggregated delta chain" class="delta-approach" /></td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 2:</span> Overview of unidirectional forward, unidirectional reverse, and bidirectional delta chain approaches,
both with and without aggregated deltas.</p>
        </figcaption>
      </figure>

      <h3 id="solution-bidirectional">Motivations for a Bidirectional Delta Chain</h3>

      <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">Experiments on the unidirectional forward aggregated delta chain approach from OSTRICH</a> <span class="references">[<a href="#ref-20">20</a>]</span>
have shown that this approach leads to linearly increasing ingestion times,
the longer the chain becomes.
This is an expected consequence of the aggregated delta approach,
as they grow in size for each new version.
The goal of this work is to investigate how these problems can be solved,
without losing the advantages of aggregated deltas with respect to query execution times.
As such, we will not achieve any lower ingestion times by reversing our delta chain,
as the additions and deletions would simply be reversed.
Instead, we aim to reduce overall storage by reducing the number of required snapshots.</p>

      <p>One straightforward way of reducing ingestion time would be
to create a new snapshot and delta chain once the ingestion time or size becomes too large.
For instance, we can lower the total ingestion time to half the original time
by splitting one delta chain into two delta chains,
or even to one third by splitting it up into three delta chains.
In the extreme, each version would be form its own snapshot,
which would lead to the independent copies storage strategy,
at the cost of increased storage size.
As such, there is a trade-off between ingestion time and storage size,
and new delta chains should only be started once ingestion times become much higher than desired.</p>

      <p>Since the creation of a snapshot can be costly, it should be avoided until absolutely necessary.
As explained in the last paragraph,
splitting up a delta chain into two separate delta chains
would lead to two snapshots, each followed by a chain of deltas.
We can however reduce the number of required snapshots
by combining the forward and reverse approaches into a <em>bidirectional</em> approach,
by allowing two sets of deltas to make use of the same snapshot.
Intuitively, one bidirectional delta chain is equivalent
to two forward delta chains,
where the second delta chain is reversed,
and the snapshots of these two chains are therefore shared,
so that it only has to be created and stored once.</p>

      <p>As such, the main advantage of a bidirectional delta chain is that it can more optimally make use of snapshots.
Instead of only allowing deltas in one direction to make use of it,
also deltas in the opposite direction can make use of it.
This is especially advantageous for aggregated deltas,
as these grow in size for longer chains.
In the scope of this research,
we continue working with a bidirectional <em>aggregated</em> delta chain
due to the non-increasing query execution times for increasing numbers of versions.</p>

      <h3 id="solution-storage">Storage Approach</h3>

      <p>As mentioned before, our storage approach builds upon that of OSTRICH.
The only difference is that OSTRICH uses a unidirectional aggregated delta chain,
and our approach uses a bidirectional aggregated delta chain.
Concretely, this means that not only deltas exist <em>after</em> the snapshot,
but also deltas exist <em>before</em> the snapshot.</p>

      <figure id="storage-overview" class="figure">
<img src="img/storage-overview.svg" alt="Storage overview" class="storage-overview" />
<figcaption>
          <p><span class="label">Fig. 1:</span> Overview of the main components of our storage approach consisting of a bidirectional aggregated delta chain.</p>
        </figcaption>
</figure>

      <p><a href="#storage-overview">Fig. 1</a> shows an overview of the main components of our storage approach,
which are analogous to the components from OSTRICH, except for the delta chain on the left side of the snapshot.
Like OSTRICH, the snapshot is stored using <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-1">1</a>]</span>,
due to its highly performant triple pattern queries and cardinality estimates,
and its high compression rate.
Furthermore, metadata about the archive is stored, containing details such as the total number of versions.
Next, each delta chain is compressed into timestamp-based B+tree indexes,
where additions and deletions are stored separately.
Each addition and deletion index is stored three times for different triple components orders (SPO, POS, OSP),
to enable efficient triple pattern queries for all possible combinations.
A shared dictionary is used to compress each triple component further.
Following the OSTRICH storage approach,
the SPO deletion index contains additional metadata about the relative position of each triple inside the snapshot.
This metadata also allows cardinality estimates for deletions to be retrieved efficiently.
To enable cardinality estimates for additions, we make use of the addition count index from OSTRICH.
For the sake of brevity, we omit further details about the components that can be found in the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">OSTRICH article</a> <span class="references">[<a href="#ref-20">20</a>]</span>.</p>

      <h3 id="solution-ingestion">Ingestion Algorithm</h3>

      <p>In this section, we introduce an algorithm to enable ingestion of new versions within our bidirectional aggregated storage approach.
For this, we make use of the ingestion algorithm from OSTRICH, which enables ingestion within a unidirectional forward aggregated delta chain.
As our approach extends from OSTRICH, we can already insert deltas <em>after</em> the snapshot,
but not yet <em>before</em> the snapshot, i.e., the reverse part of the delta chain.</p>

      <p>Our approach for constructing the reverse delta chain involves a temporary forward delta chain.
This is because we can not start building our reverse delta chain directly,
as we can not predict what triples will be in the snapshot later down the line.
For each new version, our temporary forward delta chain will be built up,
and can be queried in the meantime.
From the moment that this delta chain becomes too long, or some other treshold has been exceeded,
then an offline fix-up algorithm is triggered that will effectively <em>reverse</em> this delta chain,
and place a snapshot at the end, after which a new forward delta chain can build upon.</p>

      <p><a href="#algorithm-fixup">Algorithm 1</a> shows a sketch of our fix-up algorithm in pseudo-code.
First, the all aggregated deltas in the chain will be extracted as non-aggregated deltas by calling <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#delta-materialization">existing DM functionality in OSTRICH</a>.
We store the deletions as additions, and the additions as deletions.
Next, we create a new delta chain, and insert these reversed deltas by calling <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#ingestions">existing ingestion functionality in OSTRICH</a>.
Once ingestion is done, the existing delta chain is replaced by our new delta chain.</p>

      <figure id="algorithm-fixup" class="algorithm numbered">
<pre><code>FUNCTION fixUp(store)
</code><code># Recreate deltas, but in reverse
</code><code>additions = [] 
</code><code>deletions = []
</code><code>FOR v IN store.getVersions()
</code><code>  deletions.push(store.getAdditions(v, v + 1))
</code><code>  additions.push(store.getDeletions(v, v + 1))
</code><code># Ingest reversed deltas into a new delta chain
</code><code>newChain = store.newDeltaChain({ reversed: true })
</code><code>FOR v IN store.getVersions()
</code><code>  newChain.ingest(v, additions[v], deletions[v])
</code><code># Replace delta chain
</code><code>existingChain = store.getDeltaChain(store.getVersions())
</code><code>store.replaceChain(existingChain, newChain)
</code></pre>
<figcaption>
          <p><span class="label">Algorithm 1:</span> Fix-up algorithm for reversing an existing bidirectional aggregated delta chain.</p>
        </figcaption>
</figure>

      <p>The main advantage of this fix-up approach is that it avoids query unavailability of the archive.
The fix-up algorithm can run at any time, preferably when the server is experiencing a lower query load.
During the execution of this process, the temporary forward delta chain is still available,
so queries are still possible during this time.
Only after the fix-up process is done,
query executions will be delegated to this new reverse delta chain, 
nd the temporary forward delta chain can be deleted.</p>

      <h3 id="solution-query">Query Algorithms</h3>

      <p>In this section, we discuss triple pattern query algorithms for the three query atoms discussed in <a href="#related-work">Section 3</a> (VM, DM, VQ).
For simplicity, we assume the existence of a single (bidirectional) delta chain.
We consider multiple delta chains future work.
Just like for OSTRICH, all of these algorithms work in a streaming manner with offset support,
and offer cardinality estimators.
Below, we briefly discuss the relevant parts of the OSTRICH algorithms.
For more details, we refer to the <a property="schema:citation http://purl.org/spar/cito/cites" href="https://rdfostrich.github.io/article-jws2018-ostrich/">OSTRICH article</a> <span class="references">[<a href="#ref-20">20</a>]</span>.</p>

      <h4 id="version-materialization">Version Materialization</h4>

      <p>Version Materialization (VM) allows retrieval of triples in a given version.
In summary, <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#version-materialization">OSTRICH enables VM</a>
by either querying a snapshot directly, if the requested version is a snapshot,
or applying a given delta on the closest preceding snapshot otherwise.
In our case, a snapshot can not only exist before a delta, but also after a delta.
Nevertheless, the algorithm itself remains the same as in OSTRICH,
as the delta will have to be applied onto the snapshot in both cases.
As such, we do not discuss this VM case any further.</p>

      <h4 id="delta-materialization">Delta Materialization</h4>

      <p>Delta Materialization (DM) allows differences between to given versions to be retrieved.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#delta-materialization">OSTRICH distinguishes two cases for this</a>;
either the start version to a snapshot, or to a delta.
If the start version is a snapshot, then the result is simply a query within the delta of the end version.
Otherwise, the addition and deletion indexes for the two delta versions are iterated in a sort-merge join-like operation,
and only emits the triples that have a different addition/deletion flag for the two versions.</p>

      <p>In our bidirectional storage approach, one additional case can occur:
when the start and end version correspond to deltas in the bidirectional delta chain <em>before</em> and <em>after</em> the snapshot,
i.e., the DM query crosses the snapshot boundary.
For this, we split up our query into two queries:
a DM query from the start version until the snapshot,
and a DM query from the snapshot until the end version.
These two queries can be resolved over the two respective delta chains using <a href="https://rdfostrich.github.io/article-jws2018-ostrich/#delta-materialization">the existing DM functionality from OSTRICH</a>.
As the results from these two queries are sorted,
we can merge them in a sort-merge join way,
where triples are only emitted if they don’t exist in both streams.
<a href="#algorithm-querying-dm">Algorithm 2</a> illustrates this algorithm in pseudocode.
Following the patch notation for <a property="schema:citation http://purl.org/spar/cito/cites" href="http://darcs.net">DARCS</a> <span class="references">[<a href="#ref-29">29</a>]</span>,
with <code>o</code> being the start version, <code>e</code> being the end version and <code>s</code> being the snapshot,
our delta splitting corresponds to <sup><code>o</code></sup><code>D</code><sup><code>e</code></sup> = <sup><code>o</code></sup><code>D1</code><sup><code>s</code></sup><code>D2</code><sup><code>e</code></sup>.</p>

      <figure id="algorithm-querying-dm" class="algorithm numbered">
<pre><code>FUNCTION queryDmCase3(store, start, end)
</code><code>snapshotVersion = store.getSnapshotBetween(start, end)
</code><code>reverseStream = store.getDeltaStream(start, snapshotVersion)
</code><code>forwardStream = store.getDeltaStream(snapshotVersion, end)
</code><code>return sortMerge(reverseStream, forwardStream)
</code></pre>
<figcaption>
          <p><span class="label">Algorithm 2:</span> Delta Materialization algorithm for triple patterns that produces a triple stream
when the version range crosses the snapshot boundary.</p>
        </figcaption>
</figure>

      <p>In order to estimate the cardinality of this third case,
the same idea is followed
where the counts of the part of the delta chain before and after the snapshot are added.
Just like the existing DM cardinality estimator from OSTRICH,
this can be an overestimation, since certain triples may occur in both delta chains
that would be omitted from the final result stream.</p>

      <h4 id="version-query">Version Query</h4>

      <p>A Version Query (VQ) enables querying across all versions,
with results being annotated with the version in which they occur.
<a href="https://rdfostrich.github.io/article-jws2018-ostrich/#version-query">OSTRICH enables VQ</a>
by iterating over the snapshot for a given triple pattern in a streaming manner.
Every snapshot triple is queried within the deletion index.
For every discovered deletion, their respective version annotations are removed from the result.
If no such deletion value was found, the triple was never deleted, so the versions annotation will contain all versions of the store.
Once the snapshot stream has finished,
the addition index are iterated in a similar way,
where the version annotation of every addition triple is again updated based on its presence in the deletion index.</p>

      <p>Our case is a trivial extension of this algorithm,
where instead of checking single addition and deletion streams,
two addition and deletion streams have to be checked,
which will produce distinct version annotations.</p>

      <p>To estimate the cardinality, the OSTRICH approach can again be extended,
by adding the snapshot cardinality with the addition cardinality for both delta chains for the given triple pattern.
As some triples could occur in both delta chains, this can lead to an overestimation.</p>

    </div>
</section>

  <section id="evaluation" inlist="" rel="schema:hasPart" resource="#evaluation">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Evaluation</h2>

      <p>In this section, we evaluate our bidirectional archiving approach by comparing our implementation to native OSTRICH.</p>

      <h3 id="evaluation-implementation">Implementation</h3>

      <p>We have implemented our storage approach and query algorithms in a tool called COBRA (Change-Based Offset-Enabled Bidirectional RDF Archive).
COBRA is an extension of OSTRICH, has been implemented in C/C++, and is available under the MIT license on <a href="https://github.com/rdfostrich/cobra" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​cobra">GitHub</a>.
Our implementation uses <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328">HDT</a> <span class="references">[<a href="#ref-1">1</a>]</span> as snapshot technology,
and makes use of the highly efficient memory-mapped B+Tree implementation <a href="http://fallabs.com/kyotocabinet/" class="mandatory" data-link-text="http:/​/​fallabs.com/​kyotocabinet/​">Kyoto Cabinet</a> for storing our indexes.
The delta dictionary is encoded with <a href="http://www.gzip.org/">gzip</a>, which requires decompression during querying and ingestion.</p>

      <h3 id="evaluation-setup">Experimental Setup</h3>

      <p>In order to evaluate the ingestion and triple pattern query execution of COBRA,
we make use of the <a href="https://aic.ai.wu.ac.at/qadlod/bear.html" class="mandatory" data-link-text="https:/​/​aic.ai.wu.ac.at/​qadlod/​bear.html">BEAR benchmark</a>.
To test the scalability of our approach for datasets with few and large versions, we use the BEAR-A benchmark.
We use the ten eight versions of the BEAR-A dataset (more versions cause memory issues),
which contains 30M to 66M triples per version.
This dataset was compiled from the <a href="http://swse.deri.org/dyldo/">Dynamic Linked Data Observatory</a>.
To test for datasets with many smaller versions, we use BEAR-B with the daily and hourly granularities.
The daily dataset contains 89 versions and the hourly dataset contains 1,299 versions,
both of them have around 48K triples per version.
All experiments were performed on a 64-bit Ubuntu 14.04 machine with a 6-core 2.40 GHz CPU and 48 GB of RAM.
Our experimental setup and its raw results are available on <a href="https://github.com/rdfostrich/cobra/tree/master/Experiments/Results" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​cobra/​tree/​master/​Experiments/​Results">GitHub</a>.</p>

      <p>During our experiments, we distinguish between the following storage approaches:</p>

      <ul>
        <li><strong>OSTRICH</strong>: OSTRICH with a forward unidirectional aggregated delta chain (<a href="#evaluation-storage-approaches-ostrich">Subfig. 2.1</a>)</li>
        <li><strong>COBRA*</strong>: COBRA with a bidirectional aggregated delta chain before fix-up (<a href="#evaluation-storage-approaches-cobra-star">Subfig. 2.2</a>)</li>
        <li><strong>COBRA</strong>: COBRA with a bidirectional aggregated delta chain after fix-up (<a href="#evaluation-storage-approaches-cobra">Subfig. 2.3</a>)</li>
      </ul>

      <p>In the scope of this work, we work with at most two delta chains.
For simplicity of these experiments, we always start a new delta chain in the middle version of the dataset
(4 for BEAR-A, 45 for BEAR-B Daily, 200 for BEAR-B Hourly).
Note that for the COBRA storage approach, we assume that all versions are available beforehand,
so they can be stored out of order, starting with the middle snapshot.
In practise, this may not always be possible, which is why we report on the additional fix-up time during ingestion separately
that would be required when ingestion in order (COBRA*).</p>

      <figure id="evaluation-storage-approaches" class="figure">

<figure id="evaluation-storage-approaches-ostrich" class="subfigure">
<img src="img/approach-ostrich.png" alt="OSTRICH storage approach" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 2.1:</span> OSTRICH with a forward unidirectional aggregated delta chain</p>
          </figcaption>
</figure>

<figure id="evaluation-storage-approaches-cobra-star" class="subfigure">
<img src="img/approach-cobra-star.png" alt="COBRA* storage approach" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 2.2:</span> COBRA with a bidirectional aggregated delta chain before fix-up</p>
          </figcaption>
</figure>

<figure id="evaluation-storage-approaches-cobra" class="subfigure">
<img src="img/approach-cobra.png" alt="COBRA storage approach" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 2.3:</span> COBRA with a bidirectional aggregated delta chain after fix-up (ingested out-of-order starting with snapshot)</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 2:</span> The different storage approaches used in our experiments.</p>
        </figcaption>
</figure>

      <p>To evaluate triple pattern query performance,
we make use of the query sets provided by BEAR.
BEAR-A provides 7 query sets containing around 100 triple patterns that are further divided in high result cardinality and low result cardinality. 
BEAR-B provides two query sets that contain ?P? and ?PO queries.
We evaluate these queries as VM queries for all version, DM queries between all versions and a VQ query.
In order to minimize outliers, we replicate the queries five times and take the mean results.
Furthermore, we perform a warm-up period before the first query of each triple pattern.
Since neither OSTRICH nor COBRA support multiple snapshots for all query atoms,
we limit our experiments to OSTRICH’s unidrectional storage layout and COBRA’s bidirectional storage layout.</p>

      <h3 id="evaluation-results">Results</h3>

      <p>In this section, we discuss the results of our experiments on ingestion and query evaluation.</p>

      <h4 id="ingestion">Ingestion</h4>

      <p><a href="#ingestion-beara">Table 3</a>, <a href="#ingestion-bearbd">Table 4</a> and <a href="#ingestion-bearbh">Table 5</a> show the total storage sizes and ingestion times
for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
These tables show that COBRA less ingestion time than OSTRICH in all cases (41% less on average).
Furthermore, COBRA requires less storage space than OSTRICH for BEAR-A and BEAR-B Hourly, but not for BEAR-B Daily.
COBRA* requires more storage space than both COBRA and OSTRICH with BEAR-A, but it requires less ingestion time.
For BEAR-B Daily, OSTRICH requires less storage, but COBRA* has the lowest ingestion time.
For BEAR-B Hourly, COBRA* is lower in terms of storage size and ingestion time than both COBRA and OSTRICH.</p>

      <figure id="ingestion-beara" class="table">

        <table>
          <thead>
            <tr>
              <th>Approach</th>
              <th style="text-align: left">Storage Size (GB)</th>
              <th style="text-align: left">Ingestion Time (hours)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OSTRICH</td>
              <td style="text-align: left">3.92</td>
              <td style="text-align: left">23.66</td>
            </tr>
            <tr>
              <td>COBRA*</td>
              <td style="text-align: left">4.31</td>
              <td style="text-align: left"><em>12.92</em></td>
            </tr>
            <tr>
              <td>COBRA</td>
              <td style="text-align: left"><em>3.36</em></td>
              <td style="text-align: left">14.63</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 3:</span> Total storage size and ingestion time for BEAR-A,
with COBRA requiring the least storage size,
and COBRA* the least ingestion time.</p>
        </figcaption>
      </figure>

      <figure id="ingestion-bearbd" class="table">

        <table>
          <thead>
            <tr>
              <th>Approach</th>
              <th style="text-align: left">Storage Size (MB)</th>
              <th style="text-align: left">Ingestion Time (minutes)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OSTRICH</td>
              <td style="text-align: left"><em>19.37</em></td>
              <td style="text-align: left">6.53</td>
            </tr>
            <tr>
              <td>COBRA*</td>
              <td style="text-align: left">26.01</td>
              <td style="text-align: left"><em>3.28</em></td>
            </tr>
            <tr>
              <td>COBRA</td>
              <td style="text-align: left">28.44</td>
              <td style="text-align: left">4.24</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 4:</span> Total storage size and ingestion time for BEAR-B Daily,
with COBRA* being the smallest and fastest.</p>
        </figcaption>
      </figure>

      <figure id="ingestion-bearbh" class="table">

        <table>
          <thead>
            <tr>
              <th>Approach</th>
              <th style="text-align: left">Storage Size (MB)</th>
              <th style="text-align: left">Ingestion Time (minutes)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OSTRICH</td>
              <td style="text-align: left">61.02</td>
              <td style="text-align: left">34.47</td>
            </tr>
            <tr>
              <td>COBRA*</td>
              <td style="text-align: left"><em>46.42</em></td>
              <td style="text-align: left"><em>14.87</em></td>
            </tr>
            <tr>
              <td>COBRA</td>
              <td style="text-align: left">53.26</td>
              <td style="text-align: left">18.30</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 5:</span> Total storage size and ingestion time for BEAR-B Hourly,
with COBRA* being the smallest and fastest.</p>
        </figcaption>
      </figure>

      <p>In order to provide more details on the evolution of storage size and ingestion time,
<a href="#ingestion-size">Fig. 3</a> shows the cumulative storage size for the different datasets,
and <a href="#ingestion-time">Fig. 4</a> shows the ingestion time for these datasets.
These figures show the impact of the middle snapshots within the bidirectional chain.
For BEAR-B Daily and Hourly, the storage size significantly increases at the middle version,
but the ingestion times for all later versions significantly reset.</p>

      <figure id="ingestion-size" class="figure">

<center>
<img src="img/results/legend-ingestion.png" alt="Legend" class="results-legend" />
</center>

<figure id="ingestion-size-beara" class="subfigure">
<img src="img/results/beara-ingestion-size.png" alt="BEAR-A Ingestion Size" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 3.1:</span> BEAR-A</p>
          </figcaption>
</figure>

<figure id="ingestion-size-bearbd" class="subfigure">
<img src="img/results/bearbd-ingestion-size.png" alt="BEAR-B Daily Ingestion Size" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 3.2:</span> BEAR-B Daily</p>
          </figcaption>
</figure>

<figure id="ingestion-size-bearbh" class="subfigure">
<img src="img/results/bearbh-ingestion-size.png" alt="BEAR-B Hourly Ingestion Size" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 3.3:</span> BEAR-B Hourly</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 3:</span> Cumulative storage sizes for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
COBRA requires less storage space than OSTRICH for BEAR-A.
For BEAR-B Daily and Hourly, the middle snapshot leads to a significant increase in storage size.</p>
        </figcaption>
</figure>

      <figure id="ingestion-time" class="figure">

<center>
<img src="img/results/legend-ingestion.png" alt="Legend" class="results-legend" />
</center>

<figure id="ingestion-time-beara" class="subfigure">
<img src="img/results/beara-ingestion-time.png" alt="BEAR-A Ingestion Time" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 4.1:</span> BEAR-A</p>
          </figcaption>
</figure>

<figure id="ingestion-time-bearbd" class="subfigure">
<img src="img/results/bearbd-ingestion-time.png" alt="BEAR-B Daily Ingestion Time" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 4.2:</span> BEAR-B Daily</p>
          </figcaption>
</figure>

<figure id="ingestion-time-bearbh" class="subfigure">
<img src="img/results/bearbh-ingestion-time.png" alt="BEAR-B Hourly Ingestion Time" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 4.3:</span> BEAR-B Hourly</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 4:</span> Ingestion times per version for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
COBRA resets ingestion time from the snapshot version, while ingestion time for OSTRICH keeps increasing.</p>
        </figcaption>
</figure>

      <p>Finally, <a href="#ingestion-fixup-time">Table 6</a> show the fix-up times,
which are measured as a separate offline process.
This is the time it would take to transition from the COBRA* to COBRA storage approach,
when the versions could not be inserted out of order.
On average, this fix-up requires 3,6 times more time compared to the additional time out of order ingestion takes.</p>

      <figure id="ingestion-fixup-time" class="table">

        <table>
          <thead>
            <tr>
              <th>Dataset</th>
              <th style="text-align: left">Time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>BEAR-A</td>
              <td style="text-align: left">8.38 hours</td>
            </tr>
            <tr>
              <td>BEAR-B Daily</td>
              <td style="text-align: left">2.48 minutes</td>
            </tr>
            <tr>
              <td>BEAR-B Hourly</td>
              <td style="text-align: left">11.41 minutes</td>
            </tr>
          </tbody>
        </table>

        <figcaption>
          <p><span class="label">Table 6:</span> Fix-up duration for the different datasets.</p>
        </figcaption>
      </figure>

      <h4 id="query-evaluation">Query Evaluation</h4>

      <p><a href="#query-vm">Fig. 5</a>, <a href="#query-dm">Fig. 6</a> and <a href="#query-vq">Fig. 7</a> show the query evaluation times
for COBRA and OSTRICH for respectively VM, DM and VQ.
These figures show that for VM, COBRA is faster than OSTRICH minus a few outliers around the middle version.
For DM, COBRA is always faster than OSTRICH when querying within the first half of its delta chain.
For the second half, COBRA becomes slower, and for BEAR-B Daily even becomes slower than OSTRICH.
For VQ, COBRA is faster than OSTRICH for BEAR-B Hourly, slightly faster for BEAR-B Daily, and slower for BEAR-A.</p>

      <figure id="query-vm" class="figure">

<center>
<img src="img/results/legend-query.png" alt="Legend" class="results-legend" />
</center>

<figure id="query-vm-beara" class="subfigure">
<img src="img/results/beara-query-vm.png" alt="BEAR-A VM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 5.1:</span> BEAR-A</p>
          </figcaption>
</figure>

<figure id="query-vm-bearbd" class="subfigure">
<img src="img/results/bearbd-query-vm.png" alt="BEAR-B Daily VM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 5.2:</span> BEAR-B Daily</p>
          </figcaption>
</figure>

<figure id="query-vm-bearbh" class="subfigure">
<img src="img/results/bearbh-query-vm.png" alt="BEAR-B Hourly VM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 5.3:</span> BEAR-B Hourly</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 5:</span> Version Materialization evaluation times per version for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
For most versions, COBRA has is faster than OSTRICH.</p>
        </figcaption>
</figure>

      <figure id="query-dm" class="figure">

<center>
<img src="img/results/legend-query.png" alt="Legend" class="results-legend" />
</center>

<figure id="query-dm-beara" class="subfigure">
<img src="img/results/beara-query-dm.png" alt="BEAR-A DM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 6.1:</span> BEAR-A</p>
          </figcaption>
</figure>

<figure id="query-dm-bearbd" class="subfigure">
<img src="img/results/bearbd-query-dm.png" alt="BEAR-B Daily DM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 6.2:</span> BEAR-B Daily</p>
          </figcaption>
</figure>

<figure id="query-dm-bearbh" class="subfigure">
<img src="img/results/bearbh-query-dm.png" alt="BEAR-B Hourly DM" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 6.3:</span> BEAR-B Hourly</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 6:</span> Delta Materialization evaluation times between the middle version and all other versions
for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
For the first half of versions, COBRA is faster than OSTRICH.
For the second half of versions, COBRA becomes slower, but still faster than OSTRICH for BEAR-A and BEAR-B Hourly.</p>
        </figcaption>
</figure>

      <figure id="query-vq" class="figure">

<figure id="query-vq-beara" class="subfigure">
<img src="img/results/beara-query-vq.png" alt="BEAR-A VQ" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 7.1:</span> BEAR-A</p>
          </figcaption>
</figure>

<figure id="query-vq-bearbd" class="subfigure">
<img src="img/results/bearbd-query-vq.png" alt="BEAR-B Daily VQ" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 7.2:</span> BEAR-B Daily</p>
          </figcaption>
</figure>

<figure id="query-vq-bearbh" class="subfigure">
<img src="img/results/bearbh-query-vq.png" alt="BEAR-B Hourly VQ" class="results-triple" />
<figcaption class="for-subfigure">
            <p><span class="label">Subfig. 7.3:</span> BEAR-B Hourly</p>
          </figcaption>
</figure>

<figcaption>
          <p><span class="label">Fig. 7:</span> Version Query evaluation times across all versions for BEAR-A, BEAR-B Daily, and BEAR-B Hourly under the different storage approaches.
COBRA is faster than OSTRICH for the BEAR-B datasets, but slower for BEAR-A.</p>
        </figcaption>
</figure>

      <h3 id="evaluation-discussion">Discussion</h3>

      <p class="todo">Write me</p>

      <p>Interestingly, for BEAR-B-Hourly, it is only around version 360 that OSTRICH starts requiring more storage space than COBRA,
which confirms that long delta chains have a negative impact on both storage size and ingestion time.</p>

      <p>As such, when out of order ingestion is possible, it is preferred over fix-up.
However, since the fix-up process can happen in an offline process, this additional time is typically not a problem.</p>

    </div>
</section>

  <section id="conclusions" inlist="" rel="schema:hasPart" resource="#conclusions">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Conclusions</h2>

      <p class="todo">Conclusions and future work</p>

    </div>
</section>

</main>

<footer><section>
<h2 id="references">References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="http://www.websemanticsjournal.org/index.php/ps/article/view/328" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary RDF Representation for Publication and Exchange (HDT). Web Semantics: Science, Services and Agents on the World Wide Web. 19, 22–41 (2013).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="#rdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: RDF-3X: a RISC-style engine for RDF. Proceedings of the VLDB Endowment. 1, 647–659 (2008).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="#datasetdynamics" typeof="schema:Article">Umbrich, J., Decker, S., Hausenblas, M., Polleres, A., Hogan, A.: Towards dataset dynamics: Change frequency of Linked Open Data sources. 3rd International Workshop on Linked Data on the Web (LDOW). (2010).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="http://ceur-ws.org/Vol-1377/paper6.pdf" typeof="schema:Article">Fernández, J.D., Polleres, A., Umbrich, J.: Towards efficient archiving of Dynamic Linked Open Data. In: Debattista, J., d’Aquin, M., and Lange, C. (eds.) Proceedings of te First DIACHRON Workshop on Managing the Evolution and Preservation of the Data Web. pp. 34–49 (2015).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="http://semantic-web-journal.org/system/files/swj1814.pdf" typeof="schema:Article">Fernández, J.D., Umbrich, J., Polleres, A., Knuth, M.: Evaluating Query and Storage Strategies for RDF Archives. Semantic Web Journal. (2018).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: RDF 1.1: Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">http:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/" typeof="schema:CreativeWork">Harris, S., Seaborne, A., Prud’hommeaux, E.: SPARQL 1.1 Query Language. W3C, <a href="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/">http:/​/​www.w3.org/TR/2013/REC-sparql11-query-20130321/</a> (2013).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Mannens, E.: Exposing RDF Archives using Triple Pattern Fragments. In: Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management: Posters and Demos (2016).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="#semversion" typeof="schema:Article">Volkel, M., Winkler, W., Sure, Y., Kruk, S.R., Synak, M.: Semversion: A versioning system for RDF and ontologies. In: Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29–June 1, 2005. Proceedings (2005).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="#vcrdf" typeof="schema:Article">Cassidy, S., Ballantine, J.: Version Control for RDF Triple Stores. ICSOFT (ISDM/EHST/DC). 7, 5–12 (2007).</dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="#rwbase" typeof="schema:Article">Vander Sande, M., Colpaert, P., Verborgh, R., Coppens, S., Mannens, E., Van de Walle, R.: R&amp;Wbase: git for triples. In: Proceedings of the 6th Workshop on Linked Data on the Web (2013).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="#r43ples" typeof="schema:Article">Graube, M., Hensel, S., Urbas, L.: R43ples: Revisions for triples. In: Proceedings of the 1st Workshop on Linked Data Quality co-located with 10th International Conference on Semantic Systems (SEMANTiCS 2014) (2014).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="#vcld" typeof="schema:Article">Hauptmann, C., Brocco, M., Wörndl, W.: Scalable Semantic Version Control for Linked Data Management. In: Proceedings of the 2nd Workshop on Linked Data Quality co-located with 12th Extended Semantic Web Conference (ESWC 2015), Portorož, Slovenia (2015).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="#xrdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: x-RDF-3X: fast querying, high update rates, and consistency for RDF databases. Proceedings of the VLDB Endowment. 3, 256–263 (2010).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf" typeof="schema:Article">Gao, S., Gu, J., Zaniolo, C.: RDF-TX: A fast, user-friendly system for querying the history of RDF knowledge bases. In: Proceedings of the 19th International Conference on Extending DatabaseTechnology. pp. 269–280 (2016).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="#selfindexingarchives" typeof="schema:Article">Cerdeira-Pena, A., Farina, A., Fernández, J.D., Martı́nez-Prieto Miguel A: Self-indexing RDF archives. In: Data Compression Conference (DCC), 2016. pp. 526–535. IEEE (2016).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="#dydra" typeof="schema:Article">Anderson, J., Bendiken, A.: Transaction-time queries in dydra. In: Joint Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (MEPDaW 2016) and the 3rd Workshop on Linked Data Quality (LDQ 2016) co-located with 13th European Semantic Web Conference (ESWC 2016): MEPDaW-LDQ. pp. 11–19 (2016).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2018.08.002" typeof="schema:Article">Arndt, N., Naumann, P., Radtke, N., Martin, M., Marx, E.: Decentralized Collaborative Knowledge Management using Git. Journal of Web Semantics. (2018).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="#tailr" typeof="schema:Article">Meinhardt, P., Knuth, M., Sack, H.: TailR: a platform for preserving history on the web of data. In: Proceedings of the 11th International Conference on Semantic Systems. pp. 57–64. ACM (2015).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="https://rdfostrich.github.io/article-jws2018-ostrich/" typeof="schema:Article">Taelman, R., Vander Sande, M., Van Herwegen, J., Mannens, E., Verborgh, R.: Triple storage for random-access versioned querying of RDF archives. Journal of Web Semantics. 54, 4–28 (2019).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="#vmrdf" typeof="schema:Article">Im, D.-H., Lee, S.-W., Kim, H.-J.: A version management framework for RDF triple stores. International Journal of Software Engineering and Knowledge Engineering. 22, 85–106 (2012).</dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="#dbpedialive" typeof="schema:Article">Morsey, M., Lehmann, J., Auer, S., Stadler, C., Hellmann, S.: DBpedia and the live extraction of structured data from wikipedia. Program. 46, 157–181 (2012).</dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="#opendataportalwatch" typeof="schema:Article">Neumaier, S., Umbrich, J., Polleres, A.: Automated quality assessment of metadata across open data portals. Journal of Data and Information Quality (JDIQ). 8, 2 (2016).</dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="#jena" typeof="schema:Article">McBride, B.: Jena: A semantic web toolkit. IEEE Internet computing. 6, 55–59 (2002).</dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2017/vtpf.pdf" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R., Mannens, E.: Versioned Triple Pattern Fragments: A Low-cost Linked Data Interface Feature for Web Archives. In: Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web (2017).</dd>
  <dt id="ref-26">[26]</dt>
  <dd resource="https://rdfostrich.github.io/article-mocha-2018/" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R.: Versioned Querying with OSTRICH and Comunica in MOCHA 2018. In: Proceedings of the 5th SemWebEval Challenge at ESWC 2018 (2018).</dd>
  <dt id="ref-27">[27]</dt>
  <dd resource="https://comunica.github.io/Article-ISWC2018-Resource/" typeof="schema:Article">Taelman, R., Van Herwegen, J., Vander Sande, M., Verborgh, R.: Comunica: a Modular SPARQL Query Engine for the Web. In: Proceedings of the 17th International Semantic Web Conference (2018).</dd>
  <dt id="ref-28">[28]</dt>
  <dd resource="#rcs" typeof="schema:Article">Tichy, W.F.: RCS—a system for version control. Software: Practice and Experience. 15, 637–654 (1985).</dd>
  <dt id="ref-29">[29]</dt>
  <dd resource="http://darcs.net" typeof="schema:CreativeWork">Roundy, D.: Darcs. <a href="http://darcs.net">http:/​/​darcs.net</a> (2008).</dd>
</dl>
</section>
</footer>



</body>
</html>
